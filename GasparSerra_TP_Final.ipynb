{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "from configparser import ConfigParser\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuracion de pandas para visualizar todas las filas y columnas de un dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las variables de entorno desde el archivo variables_entorno.env\n",
    "load_dotenv(\"variables_entorno.env\")\n",
    "\n",
    "# Obtener la clave de API desde las variables de entorno\n",
    "API_KEY_ALPHA = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "# Verificar que la clave de API se hayan cargado correctamente\n",
    "if not all([API_KEY_ALPHA]):\n",
    "    raise ValueError(\"Asegúrate que la API key este en el archivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuracion parametros para mi api \n",
    "# valores por defecto para extraccion de datos\n",
    "simbolo = \"IBM\"\n",
    "intervalo = \"daily\" \n",
    "today = str(datetime.today().strftime(\"%d-%m-%Y\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "# Leo el archivo de metadata\n",
    "    try:\n",
    "        with open('metadata.json', 'r') as file:\n",
    "            metadata = json.load(file)\n",
    "        # Obtener la fecha de la última extracción\n",
    "        return (metadata['last_extraction_date'])\n",
    "    except:\n",
    "        # retorno una fecha dump que permita la ingesta de datos\n",
    "        # sabiendo que no existe un archivo de metadata\n",
    "        return (\"01-01-1900\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(df, output_path):\n",
    "# Creo un archivo con metadata cada vez que hago una ingesta\n",
    "    df = df.dropna()\n",
    "    last_extraction_date = pd.to_datetime(df.iloc[0][\"Meta Data.3. Last Refreshed\"])\n",
    "    last_extraction_date = last_extraction_date.strftime(\"%Y-%m-%d\")\n",
    "    metadata = {\n",
    "    \"last_extraction_date\": last_extraction_date\n",
    "    # aca hay un casteo implicito a string para poder guardar el json\n",
    "    }\n",
    "\n",
    "# Guardo el archivo como metadata.json\n",
    "    with open(f'{output_path}/metadata.json', 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tipo_extraccion, simbolo=None,intervalo=None,data_field=None, params=None, headers=None):\n",
    "    \"\"\"\n",
    "    Realiza una solicitud GET a una API para obtener datos.\n",
    "\n",
    "    Parámetros:\n",
    "    base_url (str): La URL base de la API.\n",
    "    endpoint (str): El endpoint de la API al que se realizará la solicitud.\n",
    "    data_field (str): Atribudo del json de respuesta donde estará la lista\n",
    "    de objetos con los datos que requerimos\n",
    "    params (dict): Parámetros de consulta para enviar con la solicitud.\n",
    "    headers (dict): Encabezados para enviar con la solicitud.\n",
    "\n",
    "    Retorna:\n",
    "    dict: Los datos obtenidos de la API en formato JSON.\n",
    "    \"\"\"\n",
    "    # Endpoint para extraccion full\n",
    "    url_e_full = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={simbolo}&apikey={API_KEY_ALPHA}'\n",
    "    # Endpoint para extraccion incremental\n",
    "    url_e_inc = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={simbolo}&interval={intervalo}&apikey={API_KEY_ALPHA}'\n",
    "    today = (datetime.now().strftime('%Y-%m-%d'))\n",
    "    if tipo_extraccion == \"full\":\n",
    "        try:\n",
    "            url = url_e_full\n",
    "        #endpoint_url = f\"{base_url}/{endpoint}\"\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()  # Levanta una excepción si hay un error en la respuesta HTTP.\n",
    "\n",
    "        # Verificar si los datos están en formato JSON.\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if data_field:\n",
    "                    data = data[data_field]\n",
    "            except:\n",
    "                print(\"El formato de respuesta no es el esperado\")\n",
    "                return None\n",
    "            return (data, tipo_extraccion)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "        # Capturar cualquier error de solicitud, como errores HTTP.\n",
    "            print(f\"La petición ha fallado. Código de error : {e}\")\n",
    "            return None\n",
    "    elif tipo_extraccion == \"incremental\":\n",
    "        url = url_e_inc\n",
    "        ultimo_dato_extraido = read_metadata()\n",
    "        if ultimo_dato_extraido < today:\n",
    "            print(\"Extrayendo los datos de hoy\")\n",
    "            try:\n",
    "                response = requests.get(url, params=params, headers=headers)\n",
    "                response.raise_for_status()  # Levanta una excepción si hay un error en la respuesta HTTP.\n",
    "\n",
    "            # Verificar si los datos están en formato JSON.\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    if data_field:\n",
    "                        data = data[data_field]\n",
    "                except:\n",
    "                    print(\"El formato de respuesta no es el esperado\")\n",
    "                    return None\n",
    "                return (data , tipo_extraccion)\n",
    "                # Verificar si los datos están en formato JSON.\n",
    "            except requests.exceptions.RequestException as e:\n",
    "            # Capturar cualquier error de solicitud, como errores HTTP.\n",
    "                print(f\"La petición ha fallado. Código de error : {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"no hay nuevos datos para extraer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(json_data, tipo_extraccion, record_path=None):\n",
    "    \"\"\"\n",
    "    Construye un DataFrame de pandas a partir de datos en formato JSON.\n",
    "\n",
    "    Parámetros:\n",
    "    json_data (dict): Los datos en formato JSON obtenidos de una API.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: Un DataFrame de pandas que contiene los datos. Y unas columnas añadidas para \n",
    "    controlar la fecha de extraccion de los datos, en caso de extraccion incremental\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.json_normalize(\n",
    "            json_data,\n",
    "            record_path)\n",
    "        if (tipo_extraccion == \"incremental\"):\n",
    "             df[\"year\"] = (pd.to_datetime(df[\"Meta Data.3. Last Refreshed\"]).dt.strftime(\"%Y\"))\n",
    "             df[\"month\"] = (pd.to_datetime(df[\"Meta Data.3. Last Refreshed\"]).dt.strftime(\"%m\"))\n",
    "             df[\"day\"] = (pd.to_datetime(df[\"Meta Data.3. Last Refreshed\"]).dt.strftime(\"%d\"))\n",
    "             print(df)\n",
    "             return df\n",
    "        elif (tipo_extraccion == \"full\"):\n",
    "            return df\n",
    "    except:\n",
    "        print(\"Los datos no están en el formato esperado\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, output_path, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Recibe un dataframe, se recomienda que haya sido convertido a un formato tabular,\n",
    "    y lo guarda en formato parquet.\n",
    "\n",
    "    Parametros:\n",
    "    df (pd.DataFrame). Dataframe a guardar.\n",
    "    output_path (str). Path + nombre del archivo\n",
    "    partition_cols (list o str). Columna/s por las cuales particionar los datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear el directorio si no existe\n",
    "    directory = os.path.dirname(output_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    df.to_parquet(\n",
    "        output_path,\n",
    "        engine=\"fastparquet\",\n",
    "        partition_cols=partition_cols\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_datos(tipo_extraccion,simbolo:None,intervalo:None):\n",
    "                  partition_cols = [\"year\",\"month\",\"day\"]\n",
    "                  if tipo_extraccion == \"incremental\":\n",
    "                      output_path = \"raw_\" + simbolo\n",
    "                      read_metadata()\n",
    "                      json = get_data(tipo_extraccion,simbolo,intervalo)\n",
    "                      df = build_table(json,tipo_extraccion)\n",
    "                      save_to_parquet(df,output_path,partition_cols)\n",
    "                      save_metadata(df,output_path)\n",
    "                  elif tipo_extraccion == \"full\":\n",
    "                        output_path = \"informacion_general_\" + simbolo\n",
    "                        json = get_data(tipo_extraccion,simbolo,intervalo)\n",
    "                        df = build_table(json,tipo_extraccion)\n",
    "                        save_to_parquet(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraccion de datos de varias empresas\n",
    "def extraer_datos_multiple(tipo_extraccion,simbolos:list,intervalo:None):\n",
    "    partition_cols = [\"year\",\"month\",\"day\"]\n",
    "    if tipo_extraccion == \"incremental\":\n",
    "                    output_path = \"multiple_incremental\"\n",
    "                    read_metadata()\n",
    "                    for simbolo in (simbolos):\n",
    "                        json = get_data(tipo_extraccion,simbolo,intervalo)\n",
    "                        globals()[simbolo] = build_table(json,tipo_extraccion)\n",
    "                    dataframes = [globals()[simbolo] for simbolo in simbolos]\n",
    "                    df = pd.concat(dataframes)\n",
    "                    df = df.dropna()\n",
    "                    save_to_parquet(df,output_path,partition_cols)\n",
    "                    save_metadata(df,output_path)\n",
    "    elif tipo_extraccion == \"full\":\n",
    "                    output_path = \"multiple_full\"\n",
    "                    for simbolo in (simbolos):\n",
    "                        json = get_data(tipo_extraccion,simbolo,intervalo)\n",
    "                        globals()[simbolo] = build_table(json,tipo_extraccion)\n",
    "                    dataframes = [globals()[simbolo] for simbolo in simbolos]\n",
    "                    df = pd.concat(dataframes)\n",
    "                    df = df.dropna()\n",
    "                    save_to_parquet(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de conexion a la base de datos relacional\n",
    "def connect_to_db(config_file, section, driverdb):\n",
    "    \"\"\"\n",
    "    Crea una conexión a la base de datos especificada en el archivo de configuración.\n",
    "\n",
    "    Parámetros:\n",
    "    config_file (str): La ruta del archivo de configuración.\n",
    "    section (str): La sección del archivo de configuración que contiene los datos de la base de datos.\n",
    "    driverdb (str): El driver de la base de datos a la que se conectará.\n",
    "\n",
    "    Retorna:\n",
    "    Un objeto de conexión a la base de datos.\n",
    "    \"\"\"\n",
    "    # driverdb = postgresql\n",
    "    # section = postgres\n",
    "    # config_file = pipeline.config\n",
    "    try:\n",
    "        # Lectura del archivo de configuración\n",
    "        parser = ConfigParser()\n",
    "        parser.read(config_file)\n",
    "\n",
    "        # Creación de un diccionario\n",
    "        # donde cargaremos los parámetros de la base de datos\n",
    "        db = {}\n",
    "        if parser.has_section(section):\n",
    "            params = parser.items(section)\n",
    "            db = {param[0]: param[1] for param in params}\n",
    "\n",
    "            # Creación de la conexión a la base de datos\n",
    "            engine = create_engine(\n",
    "                f\"{driverdb}://{db['usr']}:{db['pwd']}@{db['host']}:{db['port']}/{db['dbname']}\"\n",
    "            )\n",
    "            return engine\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"Sección {section} no encontrada en el archivo de configuración.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectarse a la base de datos: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada a la extraccion incremental\n",
    "extraer_datos(\"incremental\",\"AAPL\",\"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada a la extraccion full\n",
    "extraer_datos(\"full\",\"AAPL\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada a la extraccion multiple full\n",
    "simbolos = [\"GOOG\",\"IBM\",\"AAPL\"]\n",
    "extraer_datos_multiple(\"full\",simbolos,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada a la extraccion multiple incremental\n",
    "simbolos = [\"MSFT\",\"TSLA\",\"NVDA\"]\n",
    "extraer_datos_multiple(\"incremental\",simbolos,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segunda parte\n",
    "# leo lo archivos de los resultados de las extraccion para primero transformar y luego insertar en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"multiple_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion : limpieza de duplicados\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion: Conversión de tipos de datos de columnas\n",
    "columnas = {\n",
    "    'Symbol': \"string\",\n",
    "    'AssetType': \"category\",\n",
    "    'Description': \"string\",\n",
    "    'Name': \"string\",\n",
    "    'Exchange': \"category\",\n",
    "    'Currency': \"category\",\n",
    "    'Country': \"string\",\n",
    "    'Sector': \"category\",\n",
    "    'Industry': \"string\",\n",
    "    'MarketCapitalization': \"float\",\n",
    "    'EBITDA': \"float\",\n",
    "    'PERatio': \"float\",\n",
    "    'PEGRatio': \"float\",\n",
    "    'DividendPerShare': \"float\",\n",
    "    'DividendYield': \"float\",\n",
    "    'EPS': \"float\",\n",
    "    'ProfitMargin': \"float\",\n",
    "    'AnalystTargetPrice': \"float\",\n",
    "    'AnalystRatingStrongBuy': \"int\",\n",
    "    'AnalystRatingBuy': \"int\",\n",
    "    'AnalystRatingHold': \"int\",\n",
    "    'AnalystRatingSell': \"int\",\n",
    "    'AnalystRatingStrongSell': \"int\",\n",
    "    'EVToRevenue': \"float\",\n",
    "    'EVToEBITDA': \"float\",\n",
    "    '52WeekHigh': \"float\",\n",
    "    '52WeekLow': \"float\",\n",
    "    '50DayMovingAverage': \"float\",\n",
    "    'SharesOutstanding': \"float\",\n",
    "    'FiscalYearEnd': \"string\",\n",
    "    # conversion de columnas de tipo fecha que optimiza el almacenamiento\n",
    "    'LatestQuarter': \"datetime64[ns]\",\n",
    "    'DividendDate': \"datetime64[ns]\",\n",
    "    'ExDividendDate': \"datetime64[ns]\"\n",
    "    }\n",
    "\n",
    "df = df.astype(columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion : Eliminar columnas, por ejemplo datos que no son relevantes para determinada logica de negocio\n",
    "\n",
    "columnas_a_eliminar = [\n",
    "\"CIK\",\n",
    "\"Address\",\n",
    "\"BookValue\",\n",
    "\"RevenuePerShareTTM\",\n",
    "\"OperatingMarginTTM\",\n",
    "\"ReturnOnAssetsTTM\",\n",
    "\"ReturnOnEquityTTM\",\n",
    "\"GrossProfitTTM\",\n",
    "\"RevenueTTM\",\n",
    "\"DilutedEPSTTM\",\n",
    "\"QuarterlyEarningsGrowthYOY\",\n",
    "\"QuarterlyRevenueGrowthYOY\",\n",
    "\"TrailingPE\",\n",
    "\"ForwardPE\",\n",
    "\"PriceToSalesRatioTTM\",\n",
    "\"PriceToBookRatio\",\n",
    "\"Beta\",\n",
    "\"200DayMovingAverage\",\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columnas_a_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={   \n",
    "    \"52WeekHigh\": \"YearHigh\",\n",
    "    \"52WeekLow\":\"YearLow\",\n",
    "    \"50DayMovingAverage\": \"LastFiftyDaysMovingAverage\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion : renombrar columnas (traduccion)\n",
    "# Nota: Esta transformacion no voy a realizarla simplemente la comento\n",
    "df.rename(columns={    \n",
    "    'Symbol': \"Simbolo\",\n",
    "    'AssetType': \"Tipo_activo\",\n",
    "    'Description': \"Descripcion\",\n",
    "    'Name': \"Nombre\",\n",
    "    'Exchange': \"Indice_Bolsa\",\n",
    "    'Currency': \"Moneda\",\n",
    "    'Country': \"Pais\",\n",
    "    'Sector': \"Sector\",\n",
    "    'Industry': \"Industria\",\n",
    "    'MarketCapitalization': \"Capitalizacion_Bursatil\",\n",
    "    'DividendPerShare': \"Dividendo_x_accion\",\n",
    "    'DividendYield': \"Dividendo_porcentage\",\n",
    "    'ProfitMargin': \"Margen_ganancias\",\n",
    "    'AnalystTargetPrice': \"Precio_objetivo\",\n",
    "    'AnalystRatingStrongBuy': \"Recomendacion_compra_fuerte\",\n",
    "    'AnalystRatingBuy': \"Recomendacion_compra\",\n",
    "    'AnalystRatingHold': \"Recomendacion_mantener\",\n",
    "    'AnalystRatingSell': \"Recomendacion_venta\",\n",
    "    'AnalystRatingStrongSell': \"Recomendacion_venta_fuerte\",\n",
    "    '52WeekHigh': \"maximo_anual\",\n",
    "    '52WeekLow': \"minimo_anual\",\n",
    "    '50DayMovingAverage': \"movimiento_50_dias\",\n",
    "    'SharesOutstanding': \"numero_acciones\",\n",
    "    'FiscalYearEnd': \"fin_anio_fiscal\",\n",
    "    'LatestQuarter': \"ultimo_cuatrimestre\",\n",
    "    'DividendDate': \"fecha_dividendo\",\n",
    "    'ExDividendDate': \"anuncio_dividendo\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion:  conversión de columnas de tipo fecha al formato mas usual dia-mes-año\n",
    "columnas_fecha = [\n",
    "    'LatestQuarter',\n",
    "    'DividendDate',\n",
    "    'ExDividendDate'\n",
    "    ]\n",
    "\n",
    "for i in columnas_fecha:\n",
    "    df[i] = df[i].dt.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion: calculo de una columna como logica en base a otras dos columnas.\n",
    "df['Recomendation'] = np.where((df['AnalystRatingStrongBuy'] + df['AnalystRatingBuy']) > (df['AnalystRatingStrongSell'] + df['AnalystRatingSell']), 'Buy', 'Not Buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los datos procesados en la zona \"curada\" / procesada del data\n",
    "output_path = \"curated_multiple\"\n",
    "save_to_parquet(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conexion a la base de datos \n",
    "# parametros archivo de configuracion, motor de base de datos y librerias python usadas para la conexion\n",
    "engine = connect_to_db(\n",
    "    \"pipeline.conf\",\n",
    "    \"postgres\",\n",
    "    \"postgresql+psycopg2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo en la base de datos sql los datos de la extraccion full\n",
    "df = pd.read_parquet(\"curated_multiple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion de la tabla para insercion de datos de la extraccion full\n",
    "create_query = text(\n",
    "    \"\"\"\n",
    "    BEGIN;\n",
    "    CREATE SCHEMA IF NOT EXISTS stocks_general;\n",
    "    CREATE TABLE IF NOT EXISTS stocks_general.full(   \n",
    "    Symbol VARCHAR(4) PRIMARY KEY,\n",
    "    AssetType VARCHAR(20),\n",
    "    Name VARCHAR(100),\n",
    "    Description VARCHAR(200),\n",
    "    Exchange VARCHAR(10),\n",
    "    Currency VARCHAR(3),\n",
    "    Country VARCHAR(3),\n",
    "    Sector VARCHAR(20),\n",
    "    Industry VARCHAR(50),\n",
    "    Recomendation VARCHAR(10),\n",
    "    FiscalYearEnd VARCHAR(10),\n",
    "    MarketCapitalization REAL,\n",
    "    EBITDA REAL,\n",
    "    PERatio REAL,\n",
    "    PEGRatio REAL,\n",
    "    DividendPerShare REAL,\n",
    "    DividendYield REAL,\n",
    "    EPS REAL,\n",
    "    ProfitMargin REAL,\n",
    "    AnalystTargetPrice REAL,\n",
    "    AnalystRatingStrongBuy SMALLINT,\n",
    "    AnalystRatingBuy SMALLINT,\n",
    "    AnalystRatingHold SMALLINT,\n",
    "    AnalystRatingSell SMALLINT,\n",
    "    AnalystRatingStrongSell SMALLINT,\n",
    "    EVToRevenue REAL,\n",
    "    EVToEBITDA REAL,\n",
    "    YearHigh REAL,\n",
    "    YearLow REAL,\n",
    "    LastFiftyDaysMovingAverage REAL,\n",
    "    SharesOutstanding INTEGER,\n",
    "    DividendDate DATE,\n",
    "    ExDividendDate DATE,\n",
    "    LatestQuarter DATE\n",
    "    );\n",
    " COMMIT;\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(create_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar datos de mi extraccion full\n",
    "df.to_sql(name=\"full\", con=engine, schema=\"stocks_general\", index=False ,if_exists='append',method=\"multi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leo los datos de la extraccion incremental\n",
    "df = pd.read_parquet(\"raw_AAPL\\year=2024\\month=06\\day=21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hago unas pequeñas transformaciones a la informacion de mi extraccion incremental antes de guardar en BD\n",
    "# Eliminar columnas de metadata\n",
    "columnas_a_eliminar = [\n",
    "\"Meta Data.1. Information\",\n",
    "\"Meta Data.3. Last Refreshed\",\n",
    "\"Meta Data.4. Output Size\",\n",
    "\"Meta Data.5. Time Zone\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columnas_a_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir columnas en filas\n",
    "df = pd.melt(df,id_vars=['Meta Data.2. Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renombrar columna\n",
    "df = df.rename(columns={    \n",
    "    'Meta Data.2. Symbol': \"Symbol\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genero una columna incremental para identificar cada operacion, esta columna sera mi PK luego en la BD\n",
    "df['Id'] = range(1, len(df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion de la tabla para insercion de datos de la extraccion incremental\n",
    "create_query = text(\n",
    "    \"\"\"\n",
    "    BEGIN;\n",
    "    CREATE SCHEMA IF NOT EXISTS stocks_general;\n",
    "    CREATE TABLE IF NOT EXISTS stocks_general.incremental(   \n",
    "    Symbol VARCHAR(4),\n",
    "    variable VARCHAR(50),\n",
    "    value REAL.\n",
    "    Id INTEGER PRIMARY KEY\n",
    "    );\n",
    " COMMIT;\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(create_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar datos de mi extraccion incremental\n",
    "df.to_sql(name=\"incremental\", con=engine, schema=\"stocks_general\", index=False ,if_exists='append',method=\"multi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
